{
  "name": "glsl-tokenizer",
  "version": "0.0.8",
  "description": "r/w stream of glsl tokens",
  "main": "index.js",
  "directories": {
    "test": "test"
  },
  "scripts": {
    "test": "node test/index.js"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/chrisdickinson/glsl-tokenizer.git"
  },
  "keywords": [
    "glsl",
    "tokenizer",
    "stream"
  ],
  "author": {
    "name": "Chris Dickinson",
    "email": "chris@neversaw.us"
  },
  "license": "MIT",
  "dependencies": {
    "through": "X.X.X"
  },
  "readme": "# glsl-tokenizer\n\na [readable / writable stream](https://github.com/dominictarr/stream-spec#through-sync-writable-and-readable-aka-filter) that maps input to glsl tokens, if possible.\n\n```javascript\n\n  var fs = require('fs')\n    , tokens = require('glsl-tokenizer')()\n\n  fs.createReadStream('some.glsl')\n    .pipe(tokens)\n    .on('data', function(token) {\n      console.log(token.data, token.position, token.type)\n    })\n\n```\n\n# API\n\n### tokens = require('glsl-tokenizer')()\n\nreturn a tokenizer stream instance.\n\nemits 'data' events whenever a token is parsed with a token object as output.\n\n# tokens\n\n```javascript\n\n{ 'type': TOKEN_TYPE\n, 'data': \"string of constituent data\"\n, 'position': integer position within the data stream }\n\n```\n\nThe available token types are:\n\n* `block-comment`: `/* ... */`\n* `line-comment`: `// ... \\n`\n* `preprocessor`: `# ... \\n`\n* `operator`: Any operator. If it looks like punctuation, it's an operator.\n* `integer`\n* `float`: Optionally suffixed with `f`\n* `ident`: User defined identifier.\n* `builtin`: Builtin function\n* `keyword`\n* `whitespace`\n* `eof`: emitted on `end`; data will === `'(eof)'`.\n\n# License\n\nMIT\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/chrisdickinson/glsl-tokenizer/issues"
  },
  "homepage": "https://github.com/chrisdickinson/glsl-tokenizer",
  "_id": "glsl-tokenizer@0.0.8",
  "_from": "glsl-tokenizer@"
}
